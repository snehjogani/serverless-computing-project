{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "\n",
    "# [1] \"Story Discovery Using K-Means Clustering on News Articles\", Seangtkelley.me, 2020. [Online]. Available: https://seangtkelley.me/blog/2018/01/03/news-article-clustering. [Accessed: 30- Jul- 2020].\n",
    "\n",
    "# I have reffred following tutorial to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STANDARD OIL &lt;SRD&gt; TO FORM FINANCIAL UNIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEXAS COMMERCE BANCSHARES &lt;TCB&gt; FILES PLAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TALKING POINT/BANKAMERICA &lt;BAC&gt; EQUITY OFFER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_text\n",
       "0                                BAHIA COCOA REVIEW\n",
       "1         STANDARD OIL <SRD> TO FORM FINANCIAL UNIT\n",
       "2        TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN\n",
       "3      TALKING POINT/BANKAMERICA <BAC> EQUITY OFFER\n",
       "4  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"title_dataset.csv\",usecols =[\"title_text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>Bundesbank says it leaves credit policies unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>Bundesbank says it leaves credit policies unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>Bundesbank sets 28-day securities repurchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13258</th>\n",
       "      <td>Bundesbank sets 28-day securities repurchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>&lt;ACC CORP&gt; 3RD QTR NET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20193</th>\n",
       "      <td>&lt;ACC CORP&gt; 3RD QTR NET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>&lt;FRANKLIN CALIFORNIA TAX-FREE INCOME FUND&gt;PAYOUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19061</th>\n",
       "      <td>&lt;FRANKLIN CALIFORNIA TAX-FREE INCOME FUND&gt;PAYOUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title_text\n",
       "1948    Bundesbank says it leaves credit policies unc...\n",
       "6989    Bundesbank says it leaves credit policies unc...\n",
       "8100    Bundesbank sets 28-day securities repurchase ...\n",
       "13258   Bundesbank sets 28-day securities repurchase ...\n",
       "20476                             <ACC CORP> 3RD QTR NET\n",
       "20193                             <ACC CORP> 3RD QTR NET\n",
       "499     <FRANKLIN CALIFORNIA TAX-FREE INCOME FUND>PAYOUT\n",
       "19061   <FRANKLIN CALIFORNIA TAX-FREE INCOME FUND>PAYOUT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['title_text'].duplicated(keep=False)].sort_values('title_text').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates('title_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def token_stem(text,do_stem=True): # reffered [1]\n",
    "    tokens = []\n",
    "    \n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            token.append(word.lower())\n",
    "        \n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered.append(token)\n",
    "            \n",
    "    stems = [stemmer.stem(t) for x in filtered]\n",
    "    \n",
    "    if stems:\n",
    "        return stems\n",
    "    else:\n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_stem = []\n",
    "vocab_token = []\n",
    "for x in data['title_text']:\n",
    "    allwords = token_stem(i)\n",
    "    vocab_stem.extend(allwords)\n",
    "    allwords = token_stem(x, False)\n",
    "    vocab_token.extend(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'m\", 'abov', 'afterward', 'ai', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'ca', 'cri', 'dare', 'describ', 'did', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'gon', 'got', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'let', 'll', 'mani', 'meanwhil', 'moreov', \"n't\", 'na', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'ol', 'onc', 'onli', 'otherwis', 'ought', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'ta', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 've', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20029, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vector = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                 min_df=0.1, stop_words=custom_stopwords,\n",
    "                                 use_idf=True, tokenizer=token_stem, ngram_range=(1,3)) # reffered [1]\n",
    "\n",
    "matrix = vector.fit_transform(data['title_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['title_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:970: FutureWarning: 'precompute_distances' was deprecated in version 0.23 and will be removed in 0.25. It has no effect\n",
      "  \"effect\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:974: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(init='random', max_iter=1000, n_clusters=10, n_init=30, n_jobs=1,\n",
       "       precompute_distances='auto', random_state=10)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=10) # reffered [1]\n",
    "\n",
    "km.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: qtr,\n",
      "\n",
      "Cluster 0 titles:\n",
      "17925\n",
      "\n",
      "\n",
      "Cluster 1 words: qtr,\n",
      "\n",
      "Cluster 1 titles:\n",
      "2104\n",
      "\n",
      "\n",
      "Cluster 2 words: qtr,\n",
      "\n",
      "Cluster 2 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 3 words: qtr,\n",
      "\n",
      "Cluster 3 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 4 words: qtr,\n",
      "\n",
      "Cluster 4 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 5 words: qtr,\n",
      "\n",
      "Cluster 5 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 6 words: qtr,\n",
      "\n",
      "Cluster 6 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 7 words: qtr,\n",
      "\n",
      "Cluster 7 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 8 words: qtr,\n",
      "\n",
      "Cluster 8 titles:\n",
      "0\n",
      "\n",
      "\n",
      "Cluster 9 words: qtr,\n",
      "\n",
      "Cluster 9 titles:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"terms of cluster:\")\n",
    "\n",
    "order = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for x in range(10): # reffered [1]\n",
    "    print(\"%d word:\" % x, end='')\n",
    "    \n",
    "    for i in order[i, :100]:\n",
    "        print(' %s' % vocab_frame.loc[terms[i].split(' ')].values.tolist()[0][0], end=',')\n",
    "        \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    print(len(data[data['cluster'] == i]['title_text'].values.tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = km.predict(matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m52",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m52"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
